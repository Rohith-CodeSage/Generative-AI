{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfj4aMUDSquyH0VT8NF+st",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohith-CodeSage/Generative-AI/blob/main/2303A52198_3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1st one**"
      ],
      "metadata": {
        "id": "FqrcuBy5IoIu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7TDaakv-iYx",
        "outputId": "7af88aaf-29a2-4357-edf3-2a1cd449a818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x = 0.74, f(x) = 13.1421288\n",
            "Iteration 2: x = 0.6145552, f(x) = 11.846237994400788\n",
            "Iteration 3: x = 0.5312610807000426, f(x) = 11.245007398763406\n",
            "Iteration 4: x = 0.4693969671925482, f(x) = 10.903734822763694\n",
            "Iteration 5: x = 0.42054837262425754, f(x) = 10.686981750526822\n",
            "Iteration 6: x = 0.38043975469571134, f(x) = 10.538943463638885\n",
            "Iteration 7: x = 0.34660082495852806, f(x) = 10.43255504111426\n",
            "Iteration 8: x = 0.3174771962595419, f(x) = 10.35317021507909\n",
            "Iteration 9: x = 0.29202874676564666, f(x) = 10.292206431621567\n",
            "Iteration 10: x = 0.2695261335763863, f(x) = 10.244319008300756\n",
            "Value of x at minimum: 0.2695261335763863\n",
            "Minimum value of f(x): 10.244319008300756\n"
          ]
        }
      ],
      "source": [
        "# Define the function f(x) (not used directly in optimization)\n",
        "def f(x):\n",
        "    return 5 * x**4 + 3 * x**2 + 10\n",
        "\n",
        "# Define the derivative f'(x)\n",
        "def df(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "# Gradient Descent implementation\n",
        "def gradient_descent(start_x, learning_rate, iterations):\n",
        "    x = start_x  # Initialize x\n",
        "    for i in range(iterations):\n",
        "        grad = df(x)  # Compute the gradient\n",
        "        x -= learning_rate * grad  # Update x\n",
        "        # Print progress for demonstration\n",
        "        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}\")\n",
        "    return x\n",
        "\n",
        "# Parameters\n",
        "start_x = 1.0       # Initial guess\n",
        "learning_rate = 0.01\n",
        "iterations = 10\n",
        "\n",
        "# Run Gradient Descent\n",
        "minimum_x = gradient_descent(start_x, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Value of x at minimum:\", minimum_x)\n",
        "print(\"Minimum value of f(x):\", f(minimum_x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd one**"
      ],
      "metadata": {
        "id": "xWlTqb8PIcLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the function g(x, y)\n",
        "def g(x, y):\n",
        "    return 3 * x**2 + 5 * math.exp(-y) + 10\n",
        "\n",
        "# Define the partial derivatives of g(x, y)\n",
        "def dg_dx(x, y):\n",
        "    return 6 * x\n",
        "\n",
        "def dg_dy(x, y):\n",
        "    return -5 * math.exp(-y)\n",
        "\n",
        "# Gradient Descent implementation\n",
        "def gradient_descent(start_x, start_y, learning_rate, iterations):\n",
        "    x = start_x  # Initialize x\n",
        "    y = start_y  # Initialize y\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad_x = dg_dx(x, y)  # Compute partial derivative with respect to x\n",
        "        grad_y = dg_dy(x, y)  # Compute partial derivative with respect to y\n",
        "\n",
        "        x -= learning_rate * grad_x  # Update x\n",
        "        y -= learning_rate * grad_y  # Update y\n",
        "\n",
        "        # Print progress for demonstration\n",
        "        print(f\"Iteration {i+1}: x = {x}, y = {y}, g(x, y) = {g(x, y)}\")\n",
        "\n",
        "    return x, y  # Return the values of x and y at the minimum\n",
        "\n",
        "# Parameters\n",
        "start_x = 1.0       # Initial guess for x\n",
        "start_y = 1.0       # Initial guess for y\n",
        "learning_rate = 0.01\n",
        "iterations = 10\n",
        "\n",
        "# Run Gradient Descent\n",
        "minimum_x, minimum_y = gradient_descent(start_x, start_y, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Value of x at minimum:\", minimum_x)\n",
        "print(\"Value of y at minimum:\", minimum_y)\n",
        "print(\"Minimum value of g(x, y):\", g(minimum_x, minimum_y))\n"
      ],
      "metadata": {
        "id": "puaL1emWCL0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3rd one**"
      ],
      "metadata": {
        "id": "uEU_lY4sIhzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the sigmoid function z(x)\n",
        "def z(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid function dz/dx\n",
        "def dz_dx(x):\n",
        "    sigmoid = z(x)\n",
        "    return sigmoid * (1 - sigmoid)\n",
        "\n",
        "# Gradient Descent implementation\n",
        "def gradient_descent(start_x, learning_rate, iterations):\n",
        "    x = start_x  # Initialize x\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad = dz_dx(x)  # Compute the gradient\n",
        "        x -= learning_rate * grad  # Update x\n",
        "\n",
        "        # Print progress for demonstration\n",
        "        print(f\"Iteration {i+1}: x = {x}, z(x) = {z(x)}\")\n",
        "\n",
        "    return x  # Return the value of x at the minimum\n",
        "\n",
        "# Parameters\n",
        "start_x = 1.0       # Initial guess for x\n",
        "learning_rate = 0.1\n",
        "iterations = 10\n",
        "\n",
        "# Run Gradient Descent\n",
        "minimum_x = gradient_descent(start_x, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Value of x at minimum:\", minimum_x)\n",
        "print(\"Minimum value of z(x):\", z(minimum_x))\n"
      ],
      "metadata": {
        "id": "zk55OZF_Imq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4th one**"
      ],
      "metadata": {
        "id": "S-JBNolmI7zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Squared Error (SSE) function\n",
        "def squared_error(x_values, y_values, M, C):\n",
        "    total_error = 0\n",
        "    for i in range(len(x_values)):\n",
        "        predicted = M * x_values[i] + C\n",
        "        error = (y_values[i] - predicted) ** 2\n",
        "        total_error += error\n",
        "    return total_error\n",
        "\n",
        "# Gradient Descent implementation to minimize Squared Error\n",
        "def gradient_descent(x_values, y_values, start_M, start_C, learning_rate, iterations):\n",
        "    M = start_M  # Initialize M\n",
        "    C = start_C  # Initialize C\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Calculate gradients\n",
        "        grad_M = 0\n",
        "        grad_C = 0\n",
        "        for j in range(len(x_values)):\n",
        "            predicted = M * x_values[j] + C\n",
        "            grad_M += -2 * x_values[j] * (y_values[j] - predicted)\n",
        "            grad_C += -2 * (y_values[j] - predicted)\n",
        "\n",
        "        # Update parameters\n",
        "        M -= learning_rate * grad_M\n",
        "        C -= learning_rate * grad_C\n",
        "\n",
        "        # Print progress\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i+1}: M = {M}, C = {C}, SSE = {squared_error(x_values, y_values, M, C)}\")\n",
        "\n",
        "    return M, C\n",
        "\n",
        "# Example data (x_values and expected y_values)\n",
        "x_values = [1, 2, 3, 4, 5]\n",
        "y_values = [3, 6, 9, 12, 15]  # Linear relationship: y = 3x\n",
        "\n",
        "# Parameters\n",
        "start_M = 0.0  # Initial guess for M\n",
        "start_C = 0.0  # Initial guess for C\n",
        "learning_rate = 0.01\n",
        "iterations = 100\n",
        "\n",
        "# Run Gradient Descent\n",
        "optimal_M, optimal_C = gradient_descent(x_values, y_values, start_M, start_C, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Optimal M:\", optimal_M)\n",
        "print(\"Optimal C:\", optimal_C)\n",
        "print(\"Final Squared Error:\", squared_error(x_values, y_values, optimal_M, optimal_C))\n"
      ],
      "metadata": {
        "id": "4rl1XWfNI-kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}